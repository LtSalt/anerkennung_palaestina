###########################################################################
# 01 Scrape
###########################################################################


# Dependencies ------------------------------------------------------------

if(!"pacman" %in% installed.packages()) install.packages("pacman")
pacman::p_load(tidyverse, rvest, countrycode, janitor, lubridate)


# Scrape Table ------------------------------------------------------------

url <- "https://de.wikipedia.org/wiki/Internationale_Anerkennung_des_Staates_Pal%C3%A4stina"

url_source <- read_html(url)

states <- url_source %>% 
  html_element(".wikitable") %>% 
  html_table()


# Scrape Links ------------------------------------------------------------

links <- url_source %>% 
  html_element(".wikitable") %>% 
  html_elements("tr > td + td > span + span + a") %>% 
  html_attr("href") %>% 
  map_chr(\(href) paste0("https://de.wikipedia.org", href))

# damn you, incoherent html
# Russia and Ukraine have an additional `span` wrapper

links_complete <- links %>% 
  tibble(link = .) %>% 
  add_row(link = "https://de.wikipedia.org/wiki/Russland", .before = 39) %>% 
  add_row(link = "https://de.wikipedia.org/wiki/Ukraine", .before = 44) 


# Scrape ISO Codes --------------------------------------------------------

# Helper functions

get_iso_code <- function(url) {
  print(url)
  content <- read_html(url)
  
  content %>%
    html_element("td:contains('ISO 3166') + td") %>%
    html_text(trim = TRUE) %>%
    str_split_1(", ")
}

get_iso_codes <- function(urls) {
  n <- length(urls)
  iso_codes <- vector("list", n)
  
  for(i in 1:n) {
    iso_codes[[i]] <- get_iso_code(urls[[i]])
    Sys.sleep(1) # be nice to Wikipedia
  }
  
  iso_codes
}

# Scrape ISO codes

iso_codes <- links_complete %>% 
  mutate(iso_code = get_iso_codes(link))


# Consolidate -------------------------------------------------------------

wiki <- states %>% 
  bind_cols(iso_codes) %>% 
  unnest_wider(iso_code, names_sep = "_") %>% 
  clean_names() %>% 
  mutate(date = as.integer(str_sub(datum, -4, -1))) %>% 
  select(state = land, link, iso_code_2, date)

# Manual check 1
# wiki %>% 
#   select(state, link) %>% 
#   view()

info <- codelist %>% 
  filter(!is.na(un.name.en)) %>% 
  select(state = country.name.de, continent, eu28, iso3c) %>% 
  mutate(eu28 = ifelse(!is.na(eu28), TRUE, FALSE))

# Manual check 2
# wiki %>% anti_join(info, by = c("iso_code_2" = "iso3c"))

states_export <- wiki %>% 
  select(-state) %>% 
  full_join(info, by = c("iso_code_2" = "iso3c")) %>% 
  reframe(iso = iso_code_2,
          state,
          continent,
          recognizes = !is.na(date),
          date)


# Export ------------------------------------------------------------------

write_csv(states_export, "data/processed/states.csv")







  
  

  
  












